Methods to be explored (Initial the method you want to try)

Naive Bayes Classifier	  Vivek
Decision Tree Classifier  Vivek
Neural Networks           Ananya    
SVM                       Vivek*
Combined methods...       Vivek*
Logistic Regression       vivek
LDA                       vivek
QDA                       vivek
k-Nearest Method	Vignesh
Meta-classifiers	Vignesh
	ADABoosting		
	MultiBoost
	Bagging
	via Clustering
	via Regression
	Rotation Forest
	


* Indicates that methods are not evaluated till now 

Description (Give a summary of the method in 1-2 paragraphs)

-----------------------------------------------------------------------------------------------------------------------------

Neural Networks:

For Higgs Boson project, we used both deep and shallow neural networks. We analyzed our data using multiple networks each of which are discussed below. 


feedforwardnet : Feed Forward networks consist of a series of layers. The first layer has a connection from the network input. Each subsequent layer has a connection from the previous layer. The final layer is the output layer which produces the network output. 
The hidden layer is of size 60. The transfer function used in this layer is 'tansig' (tan-Sigmoid Transfer Function). Tansig is a neural network transfer function which calculates a layer's output from its net input. The output layer produces only 1 output ( 0 or 1). The transfer function used in this layer is 'purelin'. It is a linear transfer function which calculates the final output.


patternet: It is a Pattern Recognition Network which is trained to classify inputs from target classes. The target data should consist of a vector of 0 or 1. It contains one hidden layer of size 40. The output layer is of size 1.


cascadeforwardnet:  Cascade Forward Network is very similar to feed forward networks but includes a connection from the network input and from every previous layer to following layers. The output layer has two inputs : one from the previous hidden layer and the other from the input. The hidden layer is of size 80. The rest of the network is same as that of FeedForwardNetwork.  


Custom Neural Network:  We designed a deep neural network with 4 layers: three hidden layers, one output layer. All the hidden layers are connected to the network input.  There is no direct connection between any of the three hidden layers. The output layer takes 4 inputs, three of the inputs are outputs of each hidden layer. The output layer is also recursive layer. So it's own output is fed back as its fourth input. The network model is trained using 'trainrp' network training function which updates weight and bias according to resilient back propagation algorithm (Rprop).
The 1st hidden layer contains 20 neurons. The transfer function used in this layer is 'tansig' (tan-Sigmoid Transfer Function). The second hidden layer has 10 neurons. The transfer function used here is 'logsig'. It is log-sigmoid transfer function. The third hidden layer has 20 neurons. The transfer function used here is again 'tansig'.  The output layer produces only 1 output ( 0 or 1). The transfer function used in this layer is 'purelin'.


The number of layers in a network, number of neurons in each layer, transfer function to be used to calculate output of each layer, etc are based on heuristics. 


------------------------------------------------------------------------------------------------------------------------------


Custom Network
-----------------------
We created a deep neural network with 4 layers: three hidden layers, one output layer. The first layer is a hidden layer which is connected to the network input. The second layer is also a hidden layer which is also connected to the input.  There are no direct connection between the three hidden layers. The output layer takes 3 inputs, two of the inputs are outputs of each hidden layer. The output layer is a recursive layer. So it's output is fed back as its fourth input. The network model is trained using 'trainrp' network training function which updates weight and bias according to resilient back propagation algorithm (Rprop).

The 1st hidden layer contains 20 neurons. The transfer function used in this layer is 'tansig' (tan-Sigmoid Transfer Function). Tansig is a neural network transfer function which calculates a layer's output from its net input. The second hidden layer has 10 neurons. The transfer function used here is 'logsig'. It is log-sigmoid transfer function. The third hidden layer has 20 neurons. The transfer function used here is again 'tansig'.  The output layer produces only 1 output ( 0 or 1). The transfer function used in this layer is 'purelin' which is default. It is a linear transfer function which calculates the final output.

input data : 22 x 250000
target : 1 x 250000
Train/test split : 80-20
Hidden Layer 1 size: 20
Hidden Layer 2 size: 10
Hidden Layer 3 size: 20
Size of output layer : 1
Background: 0 , Signal : 1


Feedforward neural networks
------------------------------------
Feedforward neural networks consist of a series of layers. The first layer is the connection from the network input. The subsequent layer is a hidden layer which is connected to the input. The subsequent/ final layer is the output layer which produces the network's output. The network model is trained using 'trainscg' network training function which updates weight and bias according to the scaled conjugate gradient method.

The hidden layer contains 70 neurons.  The transfer function used in this layer is 'tansig' (tan-Sigmoid Transfer Function). Tansig is a neural network transfer function which calculates a layer's output from its net input. The output layer produces only 1 output ( 0 or 1). The transfer function used in this layer is 'purelin'. It is a linear transfer function which calculates the final output. 


input data : 22 x 250000
target : 1 x 250000
Train/test split : 80-20
Size of hidden layer(number of neurons) : 70
Size of output layer : 1
Background: 0 , Signal : 1



Cascade Forward Network
--------------------------------
These are similar to feed-forward networks but includes a connection from the input and every previous layer to following layers. The output layer has two inputs : one from the previous hidden layer and the other from the input. The rest of the network is same as that of FeedForwardNetwork. 

input data : 22 x 250000
target : 1 x 250000
Train/test split : 80-20
Size of hidden layer(number of neurons) : 70
Size of output layer : 1
Background: 0 , Signal : 1


Pattern Network
-------------------
This is a Pattern Recognition Network. These are a type of feedforward network that can be trained to classify inputs from target classes. The target data for pattern recognition networks should consist of vectors of all zero values except for a 1 in element i, where i is the class they are to represent. It contains one hidden layer of size 40. The network model is trained using 'trainrp' network training function.





