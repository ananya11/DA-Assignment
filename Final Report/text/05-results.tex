\section{Results}
\label{sec:results}

In this section, we describe the dataset used, hardware details, accuracy, and throughput results for the different classifiers.

\subsection{Dataset}

We made use of the dataset provided by Kaggle~\cite{Kaggle}. This is the cleaned up data from the original dataset provided by physicists at the UC Irvine Machine Learning Repository. This data set contains \emph{250000} instances. Each instance (row) in the dataset describes a collision event detected by the collider. Events are described by the kinematic properties (such as direction and momentum) of the particles produced in a collision. A set of 17 features describe these kinematic properties. In addition, 13 derived features that the physicists deemed important are also included in the dataset. \emph{200000} instances from the original dataset is used for \emph{training} and the remaining \emph{50000} instances is used for \emph{testing}.

\subsection{Hardware details}

All experiments were run on a MacBook Pro using a \emph{4-core} Intel Core i7 processor running at 2.5\,GHz. This machine has \emph{256\,KB} of L2 cache per core, \emph{6\,MB} of L3 cache, and \emph{16\,GB} of DDR3-1600\,MHz memory. Modeling and prediction overheads of all techniques were measured and this machine and the corresponding throughput results presented in subsequent sections.

\subsection{Accuracy Results}

\begin{table*}[t]
\centering 
\caption{Accuracy results for different models} 
\resizebox{\linewidth}{!}{%
\begin{tabular}{|l|ccc|ccc|cccc|} 
\hline 
Classifiers & Precision(s) & Recall(s) & f1-score(s) & Precision(b) & Recall(b) & f1-score(b) & Precision & Recall & f1-score & ROC Area \\
\hline
Naive Bayes & 0.66 & 0.75 & 0.7 & 0.86 & 0.8 & 0.83 & 0.79 & 0.78 & 0.79 & 0.85 \\
Decision Tree Classifier & 0.76 & 0.73 & 0.75 & 0.87 & 0.88 & 0.88 & 0.83 & 0.83 & 0.83 & 0.9 \\
Logistric Regression & 0.69 & 0.62 & 0.65 & 0.81 & 0.86 & 0.83 & 0.77 & 0.78 & 0.77 & 0.84 \\
LDA & 0.66 & 0.51 & 0.58 & 0.78 & 0.74 & 0.82 & 0.74 & 0.75 & 0.74 & 0.85 \\
QDA & 0.65 & 0.78 & 0.71 & 0.88 & 0.79 & 0.83 & 0.8 & 0.79 & 0.79 & 0.86 \\
k-nearest Neighbor & 0.72 & 0.69 & 0.72 & 0.84 & 0.86 & 0.85 & 0.8 & 0.8 & 0.8 & 0.87 \\
Custom network & 0.79 & 0.71 & 0.74 & 0.85 & 0.9 & 0.87 & 0.83 & 0.83 & 0.82 & 0.89 \\
Feed Forward Network & 0.79 & 0.74 & 0.77 & 0.87 & 0.9 & 0.88 & 0.84 & 0.84 & 0.84 & 0.91 \\
Cascade Forward Network & 0.79 & 0.74 & 0.76 & 0.87 & 0.9 & 0.88 & 0.84 & 0.85 & 0.84 & 0.91 \\
Pattern Neural Network & 0.75 & 0.71 & 0.73 & 0.85 & 0.88 & 0.87 & 0.82 & 0.82 & 0.82 & 0.9 \\
Rotation Forest & 0.8 & 0.73 & 0.76 & 0.86 & 0.91 & 0.88 & 0.84 & 0.84 & 0.84 & 0.91 \\
Boosting & 0.78 & 0.74 & 0.76 & 0.87 & 0.89 & 0.88 & 0.84 & 0.84 & 0.84 & 0.9 \\
Bagging & 0.79 & 0.74 & 0.77 & 0.87 & 0.9 & 0.88 & 0.84 & 0.85 & 0.84 & 0.91 \\
\hline
\end{tabular}}
\label{tab:Summary} 
\end{table*}

\subsubsection{Bayesian Classifiers}

As expected, the removal of correlating features had a positive impact on the accuracy of the classifier. When we decrease the threshold value of correlation for parameter removal, the accuracy decreased as the number of correlated features increased. The best performance is achieved when the dataset includes 12 features whose pairwise correlation coefficient is less than 0.6. We also observe that this method can classify background noise better than signal.

\subsubsection{Function-based Classifiers}

For this class of techniques, we explored varying the features used for classification. Our results suggest that the number of features used for classification did not affect the accuracy of the classifier. The best results obtained for this technique is shown in Table.~\ref{tab:Summary}.

\subsubsection{Tree-based Classifiers}

For the tree-based classifiers, we did not see any difference for the two criterion (gini and entropy) explored. The number of samples used to form the split, however, had a significant impact and the best performance was achieved when this value was 300. The corresponding accuracy metrics are shown in Table.~\ref{tab:Summary}.

\subsubsection{Instance-based Classifiers}

While using the raw and derived features, we did not see any notable impact of the number of neighbors on the accuracy. Also, removing irrelevant and noisy features did not help (F1-score was always around 0.70). However, simply by using principal components improved the accuracy to over 74\%. Reducing the number of principal components to four increased the accuracy to over 77.5\%. Also, increasing the number of neighbors improved the accuracy further to 80\%. The best results were obtained for number of principal components = 4 and number of neighbors = 20. The corresponding results are tabulated in Table.~\ref{tab:Summary}.

\subsubsection{Neural Network}
As mentioned before we used multiple NN technics to classify the data between a signal and a background but the classification results are very similar. Among the 4 neural networks used here, cascade forward network has the best results --  ROC curve (AUC) is 0.91, precision and recall s 0.84 and 0.85 respectively.  

We noticed that for each network model, the testing performance stops increasing with the increase in the number of neurons in the hidden layers after a certain point . Liu et al.~\cite{NN-Result} call this as the stop criterion. Beyond this point the neural network overestimates the complexity of the target problem which causes overfitting.

Recently there has been substantial interest in feed forward network with many layers. However, we restricted ourselves to only two layers (one hidden and one output layer) as we noticed an increase in overfitting when the number of layers in a feed forward network is greater than 2.  Similarly in the custom network that we designed, the neural network's performance improves when we increase the number of hidden layers from 2 to 3, the AUC is 0.89 but it is still less than other ANN models with 2 layers. 


\subsubsection{Ensemble Methods and Meta-classifiers}

\paragraph{Bagging}

Our evaluation indicates that the choice of the decision tree algorithm matters the most when applying the bagging technique. REP tree showed a 20\% better accuracy than a Decision Stump Tree. The effect of increasing the number of iterations was low, changing the bag size negligible for reasonable sizes. The best AUC value obtained was 0.91 (Corresponding f1-score was 0.84) when using REP tree with 50\% bag size, and 20 iterations. The details of the result is presented in Table.~\ref{tab:Summary}.

\paragraph{Boosting}

Among the two boosting techniques evaluated, MultiBoosting is found to be better. Our results showed REP Tree is better than Decision Stump Tree. For example, the accuracy increased from 76.8\% to 82.3\% for ADA boost, and from 77\% to 83.9\% for MultiBoost when REP Tree Classifier is used as the base classifier.

\paragraph{Rotation Forest}

Unlike bagging and boosting, the choice of the underlying decision tree classifier did not matter much. The J48 algorithm was marginally better than the REP Tree. The results for the best configuration is presented in Table.~\ref{tab:Summary}.

\paragraph{Classification via Clustering and Regression}
The regression method is comparable to other linear methods explored earlier. The classification via clustering did not work well as expected with an ROC Area of just 0.628 which is the least value obtained among all classifiers explored.


\subsection{Throughput Results}

A particle physics accelerator produces millions to billions of events in a second. The classifier has to be really fast to process that many events in real time. In our case several techniques produce similar accuracy thereby making decisions from throughput results necessary. The ensemble methods and neural networks all have similar accuracy. While it is expensive to construct a neural network, it is just an one-time cost. Ensemble methods on the other hand are relatively faster to construct, however since several models have to be evaluated before a decision is made, its prediction overhead is very high as shown in Table~\ref{tab:Throughput}. This translates to low throughput. On the other hand, neural networks show much higher throughput even though it takes longer time to construct the model.


\begin{table}[h]
\centering
\caption{Throughput results for different classifiers}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|l|r|r|r|} 
\hline 
Classifiers & Modeling(ms) & Prediction(ms) & Classifications/s \\ \hline
Naive Bayes & 86 & 11 & 4587156 \\
Decision Tree Classifier &8750 & 13 & 3937008 \\
Logistric Regression &12300 & 9 & 5561735 \\
LDA &316 & 20 & 2551020 \\
QDA & 266 & 52 & 967118 \\
k-nearest Neighbor &200 & 68000 & 735 \\
Custom network &603000 & 187 & 267380 \\
Feed Forward Network &786000 & 102 & 490196 \\
Cascade Forward Network & 1034000 & 108 & 462963 \\ 
Pattern Neural Network & 828800 & 75 & 666667 \\
Rotation Forest &362000 & 227000 & 220 \\
Boosting &87000 & 44000 & 1136 \\
Bagging & 59500 & 44000 & 1136 \\
\hline
\end{tabular}}
\label{tab:Throughput} 
\end{table}



%Models & \multicolumn{2}{|c|}{C2075} & \multicolumn{2}{|c|}{K20c}  \\ 
%\cline{2-5} 
%	   & Basic & Temp-aware & Basic & Temp-aware \\ 
%\hline
%SLR 	& 17.96 & 8.59 		& 21.67 & 9.44 	\\ 
%MLR 	& 11.59  & \textbf{4.49}	& 18.66 & 8.29 	\\ 
%MLR+I 	& 14.02 & 6.83 		& 14.74 & \textbf{6.14} 	\\ 
%QMLR 	& 14.83 & 6.42 		& 15.46 & 7.82 	\\ 
%QMLR+I 	& 19.05 & 10.31 		& 19.56 & 8.86 	\\ 
%\hline
%\end{tabular}
%\label{tab:AI-Summary} 
%\end{table}

\subsection{Discussion}

There has been much discussion about the efficacy of neural networks and ensemble methods in classification problems in HEP. While some researchers have shown that ensemble methods such as boosting is better, others have sworn that neural networks are better. Our results show that there is practically no difference between the two techniques in terms of accuracy. However, the difference in throughput is more pronounced and it makes sense to use neural networks over ensemble methods. The basic classifiers, as expected  did not show a very good accuracy with the exception of decision tree classifiers. The accuracy achieved using decision tree classifier is only marginally less than neural network, but the achieved throughput is an order of magnitude better. This makes the decision tree classifier an attractive alternative. Another important note is that the achieved ROC Area of 0.91 is better than state-of-the-art techniques~\cite{DeepNN} by over 2\%.

